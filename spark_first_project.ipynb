{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7kjaBwjYCEf"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "wfu8_21eYD6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "9wEmJ6XmYFzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def generate_large_csv(filename=\"vendas.csv\", num_rows=1_000_000):\n",
        "    \"\"\"Gera um arquivo CSV grande com dados de vendas simulados.\"\"\"\n",
        "    products = [\"Smartphone\", \"Notebook\", \"Televisao\", \"Fone de ouvido\", \"Smartwatch\", \"Tablet\", \"Monitor\", \"Webcam\"]\n",
        "    start_date = datetime(2023, 1, 1)\n",
        "\n",
        "    logger.info(f\"Gerando arquivo CSV grande: {filename} com {num_rows} linhas\")\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"id_venda,produto,quantidade,preco_unitario,data_venda\\n\")\n",
        "        for i in range(1, num_rows + 1):\n",
        "            product = random.choice(products)\n",
        "            quantity = random.randint(1, 50)\n",
        "            price = round(random.uniform(50.00, 5000.00), 2)\n",
        "\n",
        "            random_days = random.randint(0, 365 * 2)\n",
        "            sale_date = (start_date + timedelta(days=random_days)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "            f.write(f\"{i},{product},{quantity},{price},{sale_date}\\n\")\n",
        "    logger.info(\"Geração do CSV concluída!\")\n",
        "\n",
        "generate_large_csv()"
      ],
      "metadata": {
        "id": "i1deeF44YibB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ETLFunctions\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "logger.info(\"SparkSession inicializada com sucesso!\")"
      ],
      "metadata": {
        "id": "_OFa8C6MY0qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
        "\n",
        "\n",
        "def extract(spark_session: SparkSession, input_path: str):\n",
        "    logger.info(f\"\\n--- Iniciando Extração de dados de: {input_path} ---\")\n",
        "    try:\n",
        "        schema = StructType([\n",
        "            StructField(\"id_venda\", IntegerType(), True),\n",
        "            StructField(\"produto\", StringType(), True),\n",
        "            StructField(\"quantidade\", IntegerType(), True),\n",
        "            StructField(\"preco_unitario\", DoubleType(), True),\n",
        "            StructField(\"data_venda\", DateType(), True)\n",
        "        ])\n",
        "        df = spark_session.read \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .schema(schema) \\\n",
        "            .csv(input_path)\n",
        "\n",
        "        logger.info(f\"Dados extraídos. Número de linhas: {df.count()}\")\n",
        "        logger.info(\"Schema dos dados extraídos:\")\n",
        "        df.printSchema()\n",
        "        logger.info(\"Primeiras 5 linhas dos dados extraídos:\")\n",
        "        df.show(5)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logger.info(f\"Erro na fase de Extração: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "uguZHxdoY4Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def transform(df_input):\n",
        "    logger.info(\"\\n--- Iniciando Transformação de dados ---\")\n",
        "    if df_input is None:\n",
        "        logger.error(\"DataFrame de entrada é nulo\")\n",
        "        raise Exception(\"DataFrame de entrada é nulo\")\n",
        "\n",
        "    df_transformed = df_input.withColumn(\n",
        "        \"valor_total_venda\",\n",
        "        col(\"quantidade\") * col(\"preco_unitario\")\n",
        "    )\n",
        "\n",
        "    logger.info(\"Coluna 'valor_total_venda' adicionada.\")\n",
        "    logger.info(\"Schema do DataFrame transformado:\")\n",
        "    df_transformed.printSchema()\n",
        "    logger.info(\"Primeiras 5 linhas do DataFrame transformado:\")\n",
        "    df_transformed.show(5)\n",
        "\n",
        "    df_sales_by_product = df_transformed.groupBy(\"produto\") \\\n",
        "        .agg(F.sum(\"valor_total_venda\").alias(\"total_vendas_produto\"))\n",
        "\n",
        "    logger.info(\"\\nVendas totais por produto (amostra):\")\n",
        "    df_sales_by_product.show(5, truncate=False)\n",
        "\n",
        "    return df_transformed, df_sales_by_product\n"
      ],
      "metadata": {
        "id": "TXeEhPkhY57t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def load(df_to_load, output_path: str):\n",
        "    logger.info(f\"\\n--- Iniciando Carga de dados para: {output_path} ---\")\n",
        "\n",
        "    if df_to_load is None:\n",
        "        logger.error(\"DataFrame principal para carregar é nulo, pulando carga\")\n",
        "        raise Exception(\"DataFrame principal para carregar é nulo, pulando carga\")\n",
        "\n",
        "    if os.path.exists(output_path):\n",
        "        shutil.rmtree(output_path)\n",
        "        logger.info(f\"Diretório '{output_path}' removido.\")\n",
        "\n",
        "    try:\n",
        "        df_to_load.write \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .parquet(output_path)\n",
        "        logger.info(f\"Dados salvos com sucesso em '{output_path}'\")\n",
        "\n",
        "        logger.info(f\"\\nVerificando algumas linhas dos dados salvos em '{output_path}':\")\n",
        "        df_verificacao = spark.read.parquet(output_path)\n",
        "        df_verificacao.show(5)\n",
        "        logger.info(f\"Número total de linhas no Parquet salvo: {df_verificacao.count()}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erro na fase de Carga do DataFrame principal: {e}\")\n",
        "        raise Exception(f\"Erro na fase de Carga do DataFrame principal: {e}\")"
      ],
      "metadata": {
        "id": "d9Px8sX5ZQek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_csv_path = \"vendas.csv\"\n",
        "output_main_parquet_path = \"vendas_processadas_principais.parquet\"\n",
        "output_agg_parquet_path = \"vendas_agregadas_por_produto.parquet\"\n",
        "\n",
        "df_raw = extract(spark, input_csv_path)\n",
        "\n",
        "if df_raw is not None:\n",
        "    df_processed, df_aggregated = transform(df_raw)\n",
        "\n",
        "    if df_processed is not None:\n",
        "        load(df_processed, output_main_parquet_path)\n",
        "        load(df_aggregated, output_agg_parquet_path)\n",
        "    else:\n",
        "        logger.info(\"Transformação não gerou DataFrame, pulando carga.\")\n",
        "else:\n",
        "    logger.info(\"Extração não gerou DataFrame, pulando transformação e carga.\")\n",
        "\n",
        "spark.stop()\n",
        "logger.info(\"\\nSparkSession encerrada.\")"
      ],
      "metadata": {
        "id": "k1JgML6uZr9q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}